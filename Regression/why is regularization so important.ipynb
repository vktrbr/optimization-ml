{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb3e555",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c0edb9",
   "metadata": {},
   "source": [
    "#### Make regression data and columns in 0, 1, 2, ..., n degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd936bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_regression(6, 1, noise=10)\n",
    "degree = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e79f222",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pol = PolynomialFeatures(degree).fit_transform(X)\n",
    "\n",
    "X_pol[:3, :3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568a0796",
   "metadata": {},
   "source": [
    "#### Make regression without regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591255a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(w: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    loss function without any regularization\n",
    "    \n",
    "    :param w: weight matrix\n",
    "    :return: sum of squared errors loss \n",
    "    \"\"\"\n",
    "    \n",
    "    return ((X_pol @ w - y) ** 2).sum()\n",
    "\n",
    "\n",
    "# find the solution of the equation x @ w = y by minimization of the loss function\n",
    "w_opt = minimize(loss_func, [1] * X_pol.shape[1])['x']\n",
    "\n",
    "\n",
    "# data for lineplot\n",
    "min_max_delta = 0.05 * (X.max() - X.min()) # constant for predicting values to the left and to the right \n",
    "\n",
    "x_axis = np.linspace(X.min() - min_max_delta,\n",
    "                     X.max() + min_max_delta,\n",
    "                     50)\n",
    "\n",
    "y_axis = PolynomialFeatures(degree).fit_transform(x_axis.reshape(-1, 1)) @ w_opt\n",
    "\n",
    "# drawing plot\n",
    "fig = px.scatter({'x': X.reshape(-1), 'y': y}, x='x', y='y')\n",
    "fig = fig.add_scatter(x=x_axis, y=y_axis, mode='lines',showlegend=False)\n",
    "fig.show()\n",
    "\n",
    "# let's print the coefficients before the powers of x\n",
    "print(np.round(w_opt, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e953db2",
   "metadata": {},
   "source": [
    "#### Regression with L1 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33be424d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func_l1(w):\n",
    "    \"\"\"\n",
    "    loss function with L1 regularization\n",
    "    \n",
    "    :param w: weight matrix\n",
    "    :return: sum of squared errors loss \n",
    "    \"\"\"\n",
    "    \n",
    "    return (1 / X_pol.shape[0]) * ((X_pol @ w - y) ** 2).sum() +  abs(w).sum()\n",
    "\n",
    "# find the solution by minimization of the loss function\n",
    "w_opt = minimize(loss_func_l1, [1] * X_pol.shape[1])['x']\n",
    "\n",
    "# data for lineplot\n",
    "min_max_delta = 0.05 * (X.max() - X.min()) # constant for predicting values to the left and to the right \n",
    "\n",
    "x_axis = np.linspace(X.min() - min_max_delta,\n",
    "                     X.max() + min_max_delta,\n",
    "                     50)\n",
    "y_axis = PolynomialFeatures(degree).fit_transform(x_axis.reshape(-1, 1)) @ w_opt\n",
    "# drawing plot\n",
    "\n",
    "fig = px.scatter({'x': X.reshape(-1), 'y': y}, x='x', y='y')\n",
    "fig = fig.add_scatter(x=x_axis, y=y_axis, mode='lines',showlegend=False)\n",
    "fig.show()\n",
    "\n",
    "# let's print the coefficients before the powers of x\n",
    "print(np.round(w_opt, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb9728a",
   "metadata": {},
   "source": [
    "#### Regression with L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69ea6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func_l2(w):\n",
    "    \"\"\"\n",
    "    loss function with L2 regularization\n",
    "    \n",
    "    :param w: weight matrix\n",
    "    :return: sum of squared errors loss \n",
    "    \"\"\"\n",
    "    return ((X_pol @ w - y) ** 2).sum() + (w ** 2).sum()\n",
    "\n",
    "# find the solution by minimization of the loss function\n",
    "w_opt = minimize(loss_func_l2, [1] * X_pol.shape[1])['x']\n",
    "\n",
    "# data for lineplot\n",
    "min_max_delta = 0.05 * (X.max() - X.min()) # constant for predicting values to the left and to the right \n",
    "\n",
    "x_axis = np.linspace(X.min() - min_max_delta,\n",
    "                     X.max() + min_max_delta,\n",
    "                     50)\n",
    "y_axis = PolynomialFeatures(degree).fit_transform(x_axis.reshape(-1, 1)) @ w_opt\n",
    "# drawing plot\n",
    "\n",
    "fig = px.scatter({'x': X.reshape(-1), 'y': y}, x='x', y='y')\n",
    "fig = fig.add_scatter(x=x_axis, y=y_axis, mode='lines',showlegend=False)\n",
    "fig.show()\n",
    "\n",
    "# let's print the coefficients before the powers of x\n",
    "print(np.round(w_opt, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913646c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
